# $t@$h
# This program does NOT modify your system in any way
# Uses ensemble learning to detect embedded nim files
# Working on ways to improve detection. See comments at
# the bottom for the play by play. Naive bayes + decision tree
import os
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Function to read files from a directory
def read_files_from_directory(directory):
    contents = []
    for root, dirs, files in os.walk(directory):
        for file in files:
            try:
                with open(os.path.join(root, file), 'r', encoding='utf-8', errors='ignore') as f:
                    contents += f.readlines()
            except Exception as e:
                print(f"Error reading {file}: {e}")
    return contents

nim_directory = 'nim' # Path to training data positives
non_nim_directory = 'non_nim' # Path to training data negatives

nim_samples = read_files_from_directory(nim_directory)
non_nim_samples = read_files_from_directory(non_nim_directory)

# Combine samples, create labels
samples = nim_samples + non_nim_samples
labels = [1] * len(nim_samples) + [0] * len(non_nim_samples)

# Vectorization
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(samples)

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)

# Define base
base_learners = [
    ('dt', DecisionTreeClassifier(random_state=42)),
    ('nb', MultinomialNB())
]

# Define meta
meta_learner = LogisticRegression(random_state=42)

# Stack
stacked_clf = StackingClassifier(estimators=base_learners, final_estimator=meta_learner)

# Train model
stacked_clf.fit(X_train, y_train)

# Predict and evaluate
y_pred = stacked_clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f'Stacked Model Accuracy: {accuracy:.2%}')

# Results are:
# stashakkori$ python3 detect.py
# Stacked Model Accuracy: 99.47%
# dijkstra.cpp: Non-Nim
# prime_number.py: Non-Nim
# merge-sort.cpp: Non-Nim
# shellcode_bin.nim: Nim
# quick_sort.py: Non-Nim
# longest-common-subsequence.cpp: Non-Nim
# reverse_string.py: Non-Nim
# excel_com_bin.nim: Nim
# minidump_bin.nim: Nim
# connection.txt: Non-Nim
# prime-number.cpp: Non-Nim
# minimum_spanning_tree.py: Non-Nim
# roman_numeral.py: Non-Nim
# rot13.py: Non-Nim
# selection-sort.cpp: Non-Nim
# main.nim: Non-Nim
# iran_server.nim: Non-Nim
# recorder.nim: Nim
# reverse-string.cpp: Non-Nim
# dns_resolve.nim: Nim
# pipe.nim: Nim
# transpose_matrix.py: Non-Nim
# quick-sort.cpp: Non-Nim
# rot13.txt: Non-Nim
# depth-first-search.cpp: Non-Nim
# palindromic-number.cpp: Non-Nim
# foreign_server.nim: Non-Nim
# globals.txt: Non-Nim
# roman-numeral.cpp: Non-Nim

# Results indicate maybe some overfitting to extension
# As some false negatives such as:
#    foreign_server.nim: Non-Nim (should be Nim)
#    iran_server.nim: Non-Nim (should be Nim)
#    connection.txt: Non-Nim (should be Nim)
# Next steps:
#    1. Add more nim files to nim training data set
#    2. Make all file extensions .txt and retrain
